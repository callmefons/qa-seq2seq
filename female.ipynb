{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1.10.1\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pandas.io import gbq\n",
    "from google.cloud import bigquery\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import MeCab\n",
    "\n",
    "print(tf.executing_eagerly())\n",
    "print(tf.__version__)\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 668217339103862279\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11266549351\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 10383044686059752687\n",
      "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load data from google BigQuery\n",
    "\n",
    "# project_id = 'robot-personnel'\n",
    "# query = \"\"\" SELECT * FROM qa_data.free_answer WHERE gender == '女性' \"\"\"\n",
    "\n",
    "# dataset = gbq.read_gbq(query, project_id)\n",
    "# dataset.to_csv('./data/female.csv', sep=',', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'theme_id', 'question', 'answer_val', 'answer_id', 'age',\n",
       "       'gender', 'prefecture'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "df = pd.read_csv('./data/female.csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>theme_id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>prefecture</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>いい夫婦</td>\n",
       "      <td>Q1.あなたが理想とする“いい夫婦”のイメージに当てはまるものをお選びください。(MA)16...</td>\n",
       "      <td>お互いに支えあえる</td>\n",
       "      <td>28</td>\n",
       "      <td>女性</td>\n",
       "      <td>愛知県</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>いい夫婦</td>\n",
       "      <td>Q1.あなたが理想とする“いい夫婦”のイメージに当てはまるものをお選びください。(MA)16...</td>\n",
       "      <td>一緒にいて疲れない、空気のような感じ</td>\n",
       "      <td>53</td>\n",
       "      <td>女性</td>\n",
       "      <td>三重県</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>いい夫婦</td>\n",
       "      <td>Q1.あなたが理想とする“いい夫婦”のイメージに当てはまるものをお選びください。(MA)16...</td>\n",
       "      <td>些細なことでもありがとうと口にだす</td>\n",
       "      <td>73</td>\n",
       "      <td>女性</td>\n",
       "      <td>福岡県</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>いい夫婦</td>\n",
       "      <td>Q1.あなたが理想とする“いい夫婦”のイメージに当てはまるものをお選びください。(MA)16...</td>\n",
       "      <td>個人、個人の時間を大事にする</td>\n",
       "      <td>69</td>\n",
       "      <td>女性</td>\n",
       "      <td>東京都</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>いい夫婦</td>\n",
       "      <td>Q1.あなたが理想とする“いい夫婦”のイメージに当てはまるものをお選びください。(MA)16...</td>\n",
       "      <td>家族の問題を分かち合える</td>\n",
       "      <td>47</td>\n",
       "      <td>女性</td>\n",
       "      <td>東京都</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  theme_id                                           question  \\\n",
       "0     いい夫婦  Q1.あなたが理想とする“いい夫婦”のイメージに当てはまるものをお選びください。(MA)16...   \n",
       "1     いい夫婦  Q1.あなたが理想とする“いい夫婦”のイメージに当てはまるものをお選びください。(MA)16...   \n",
       "2     いい夫婦  Q1.あなたが理想とする“いい夫婦”のイメージに当てはまるものをお選びください。(MA)16...   \n",
       "3     いい夫婦  Q1.あなたが理想とする“いい夫婦”のイメージに当てはまるものをお選びください。(MA)16...   \n",
       "4     いい夫婦  Q1.あなたが理想とする“いい夫婦”のイメージに当てはまるものをお選びください。(MA)16...   \n",
       "\n",
       "               answer  age gender prefecture  \n",
       "0           お互いに支えあえる   28     女性        愛知県  \n",
       "1  一緒にいて疲れない、空気のような感じ   53     女性        三重県  \n",
       "2   些細なことでもありがとうと口にだす   73     女性        福岡県  \n",
       "3      個人、個人の時間を大事にする   69     女性        東京都  \n",
       "4        家族の問題を分かち合える   47     女性        東京都  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select column and rename\n",
    "df = df[['theme_id', 'question', 'answer_val', 'age', 'gender', 'prefecture']]\n",
    "df = df.rename(columns={'answer_val': 'answer'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference : https://gist.github.com/ryanmcgrath/982242\n",
    "# UNICODE RANGE : DESCRIPTION \n",
    "# 3000-303F : punctuation\n",
    "# 3040-309F : hiragana\n",
    "# 30A0-30FF : katakana\n",
    "# FF00-FFEF : Full-width roman + half-width katakana\n",
    "# 4E00-9FAF : Common and uncommon kanji\n",
    "\n",
    "def clean_question(text):\n",
    "    text = re.split(r'[。]', text)\n",
    "    text = clean_text(text[0]+'。')\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    unicode = u\"([^\\u3000-\\u303F\\u3040-\\u309F\\u30A0-\\u30FF\\uFF00-\\uFFEF\\u4E00-\\u9FAF])\"\n",
    "    text = re.sub(unicode, \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>theme_id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>prefecture</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>いい夫婦</td>\n",
       "      <td>あなたが理想とするいい夫婦のイメージに当てはまるものをお選びください。</td>\n",
       "      <td>お互いに支えあえる</td>\n",
       "      <td>28</td>\n",
       "      <td>女性</td>\n",
       "      <td>愛知県</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>いい夫婦</td>\n",
       "      <td>あなたが理想とするいい夫婦のイメージに当てはまるものをお選びください。</td>\n",
       "      <td>一緒にいて疲れない、空気のような感じ</td>\n",
       "      <td>53</td>\n",
       "      <td>女性</td>\n",
       "      <td>三重県</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>いい夫婦</td>\n",
       "      <td>あなたが理想とするいい夫婦のイメージに当てはまるものをお選びください。</td>\n",
       "      <td>些細なことでもありがとうと口にだす</td>\n",
       "      <td>73</td>\n",
       "      <td>女性</td>\n",
       "      <td>福岡県</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>いい夫婦</td>\n",
       "      <td>あなたが理想とするいい夫婦のイメージに当てはまるものをお選びください。</td>\n",
       "      <td>個人、個人の時間を大事にする</td>\n",
       "      <td>69</td>\n",
       "      <td>女性</td>\n",
       "      <td>東京都</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>いい夫婦</td>\n",
       "      <td>あなたが理想とするいい夫婦のイメージに当てはまるものをお選びください。</td>\n",
       "      <td>家族の問題を分かち合える</td>\n",
       "      <td>47</td>\n",
       "      <td>女性</td>\n",
       "      <td>東京都</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  theme_id                             question              answer  age  \\\n",
       "0     いい夫婦  あなたが理想とするいい夫婦のイメージに当てはまるものをお選びください。           お互いに支えあえる   28   \n",
       "1     いい夫婦  あなたが理想とするいい夫婦のイメージに当てはまるものをお選びください。  一緒にいて疲れない、空気のような感じ   53   \n",
       "2     いい夫婦  あなたが理想とするいい夫婦のイメージに当てはまるものをお選びください。   些細なことでもありがとうと口にだす   73   \n",
       "3     いい夫婦  あなたが理想とするいい夫婦のイメージに当てはまるものをお選びください。      個人、個人の時間を大事にする   69   \n",
       "4     いい夫婦  あなたが理想とするいい夫婦のイメージに当てはまるものをお選びください。        家族の問題を分かち合える   47   \n",
       "\n",
       "  gender prefecture  \n",
       "0     女性        愛知県  \n",
       "1     女性        三重県  \n",
       "2     女性        福岡県  \n",
       "3     女性        東京都  \n",
       "4     女性        東京都  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning the questions and answers\n",
    "df['question'] = df['question'].apply(lambda x: clean_question(str(x)))\n",
    "df['answer'] = df['answer'].apply(lambda x: clean_text(str(x)))\n",
    "\n",
    "# remove empty cell\n",
    "filter = df[\"answer\"] != \"\"\n",
    "df = df[filter].reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>theme_id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>prefecture</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>いい夫婦</td>\n",
       "      <td>あなたが理想とするいい夫婦のイメージに当てはまるものをお選びください。</td>\n",
       "      <td>お互いに支えあえる</td>\n",
       "      <td>28</td>\n",
       "      <td>女性</td>\n",
       "      <td>愛知県</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>いい夫婦</td>\n",
       "      <td>あなたが理想とするいい夫婦のイメージに当てはまるものをお選びください。</td>\n",
       "      <td>一緒にいて疲れない、空気のような感じ</td>\n",
       "      <td>53</td>\n",
       "      <td>女性</td>\n",
       "      <td>三重県</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>いい夫婦</td>\n",
       "      <td>あなたが理想とするいい夫婦のイメージに当てはまるものをお選びください。</td>\n",
       "      <td>些細なことでもありがとうと口にだす</td>\n",
       "      <td>73</td>\n",
       "      <td>女性</td>\n",
       "      <td>福岡県</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>いい夫婦</td>\n",
       "      <td>あなたが理想とするいい夫婦のイメージに当てはまるものをお選びください。</td>\n",
       "      <td>個人、個人の時間を大事にする</td>\n",
       "      <td>69</td>\n",
       "      <td>女性</td>\n",
       "      <td>東京都</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>いい夫婦</td>\n",
       "      <td>あなたが理想とするいい夫婦のイメージに当てはまるものをお選びください。</td>\n",
       "      <td>家族の問題を分かち合える</td>\n",
       "      <td>47</td>\n",
       "      <td>女性</td>\n",
       "      <td>東京都</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  theme_id                             question              answer  age  \\\n",
       "0     いい夫婦  あなたが理想とするいい夫婦のイメージに当てはまるものをお選びください。           お互いに支えあえる   28   \n",
       "1     いい夫婦  あなたが理想とするいい夫婦のイメージに当てはまるものをお選びください。  一緒にいて疲れない、空気のような感じ   53   \n",
       "2     いい夫婦  あなたが理想とするいい夫婦のイメージに当てはまるものをお選びください。   些細なことでもありがとうと口にだす   73   \n",
       "3     いい夫婦  あなたが理想とするいい夫婦のイメージに当てはまるものをお選びください。      個人、個人の時間を大事にする   69   \n",
       "4     いい夫婦  あなたが理想とするいい夫婦のイメージに当てはまるものをお選びください。        家族の問題を分かち合える   47   \n",
       "\n",
       "  gender prefecture  \n",
       "0     女性        愛知県  \n",
       "1     女性        三重県  \n",
       "2     女性        福岡県  \n",
       "3     女性        東京都  \n",
       "4     女性        東京都  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering out the questions and answers that are too short or too long\n",
    "MAX_LENGTH = 50\n",
    "df = df[df['question'].map(len) < MAX_LENGTH]\n",
    "df = df[df['answer'].map(len) < MAX_LENGTH]\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64973, 6)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "num_examples = len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "    w = clean_text(w)\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(sentence):\n",
    "    \n",
    "    sentence = sentence.split('<start>')[1]\n",
    "    sentence = sentence.split('<end>')[0]\n",
    "    \n",
    "    temp = ''\n",
    "    tagger = MeCab.Tagger('')\n",
    "    tagger.parse('') \n",
    "    \n",
    "    node = tagger.parseToNode(sentence)\n",
    "    while node:\n",
    "        word = str(node.surface)\n",
    "        node = node.next\n",
    "            \n",
    "        temp += word + ' '\n",
    "        \n",
    "    return '<start> ' + temp + ' <end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return word pairs in the format: [QUESTION, ANSWER]\n",
    "\n",
    "def create_dataset(num_examples):\n",
    "\n",
    "    word_pairs = [['' for x in range(2)] for n in range(num_examples)] \n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        word_pairs[index][0] = preprocess_sentence(row['question']) \n",
    "        word_pairs[index][1] = preprocess_sentence(row['answer'])\n",
    "        \n",
    "        if index == num_examples - 1: break\n",
    "    \n",
    "    return word_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa (e.g., 5 -> \"dad\") \n",
    "class LanguageIndex():\n",
    "    def __init__(self, lang):\n",
    "        self.lang = lang\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = set()\n",
    "\n",
    "        self.create_index()\n",
    "\n",
    "    def create_index(self):\n",
    "        for phrase in self.lang:\n",
    "            self.vocab.update(tokenizer(phrase).split(' '))\n",
    "        \n",
    "        self.vocab = sorted(self.vocab)\n",
    "        self.word2idx['<pad>'] = 0\n",
    "        self.word2idx['<unk>'] = 1\n",
    "        \n",
    "        for index, word in enumerate(self.vocab):\n",
    "            self.word2idx[word] = index + 2\n",
    "\n",
    "        for word, index in self.word2idx.items():\n",
    "            self.idx2word[index] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "def load_dataset(num_examples):\n",
    "    # creating cleaned input, output pairs\n",
    "    pairs = create_dataset(num_examples)\n",
    "\n",
    "    # index word using the class defined above    \n",
    "    inp_lang = LanguageIndex(q for q, a in pairs)\n",
    "    targ_lang = LanguageIndex(a for q, a in pairs)\n",
    "    \n",
    "    # Vectorize the input and target languages\n",
    "    # Question \n",
    "    input_tensor = [[inp_lang.word2idx[q] if q in inp_lang.word2idx else inp_lang.word2idx['<unk>'] for q in tokenizer(q).split(' ') if q] for q, a in pairs]\n",
    "    \n",
    "    # Answer \n",
    "    target_tensor = [[targ_lang.word2idx[a] if a in targ_lang.word2idx else targ_lang.word2idx['<unk>'] for a in tokenizer(a).split(' ') if a] for q, a in pairs]\n",
    "    \n",
    "    # Calculate max_length of input and output tensor\n",
    "    # Here, we'll set those to the longest sentence in the dataset\n",
    "    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
    "    \n",
    "    # Padding the input and output tensor to the maximum length\n",
    "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
    "                                                                 maxlen=max_length_inp,\n",
    "                                                                 padding='post')\n",
    "    \n",
    "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
    "                                                                  maxlen=max_length_tar, \n",
    "                                                                  padding='post')\n",
    "    \n",
    "    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limit the size of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 38, 892, 19503)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_inp, max_length_targ, len(inp_lang.vocab), len(targ_lang.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51978, 51978, 12995, 12995)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a tf.data dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 32\n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dim = 64\n",
    "units = 128\n",
    "vocab_inp_size = len(inp_lang.word2idx)\n",
    "vocab_tar_size = len(targ_lang.word2idx)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(units):\n",
    "    # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n",
    "    # the code automatically does that.\n",
    "    if tf.test.is_gpu_available():\n",
    "        return tf.keras.layers.CuDNNGRU(units, \n",
    "                                        return_sequences=True, \n",
    "                                        return_state=True, \n",
    "                                        recurrent_initializer='glorot_uniform')\n",
    "    else:\n",
    "        return tf.keras.layers.GRU(units, \n",
    "                                   return_sequences=True, \n",
    "                                   return_state=True, \n",
    "                                   recurrent_activation='sigmoid', \n",
    "                                   recurrent_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.enc_units)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)        \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        # score shape == (batch_size, max_length, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoints (Object-based saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints/female/low'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = './logs/female/low'\n",
    "\n",
    "summary_writer = tf.contrib.summary.create_file_writer(log_dir, flush_millis=10000)\n",
    "summary_writer.set_as_default()\n",
    "global_step = tf.train.get_or_create_global_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.4771\n",
      "Epoch 1 Batch 100 Loss 1.3727\n",
      "Epoch 1 Batch 200 Loss 1.8801\n",
      "Epoch 1 Batch 300 Loss 1.4072\n",
      "Epoch 1 Batch 400 Loss 1.1826\n",
      "Epoch 1 Batch 500 Loss 1.4412\n",
      "Epoch 1 Batch 600 Loss 1.2582\n",
      "Epoch 1 Batch 700 Loss 1.4105\n",
      "Epoch 1 Batch 800 Loss 1.2908\n",
      "Epoch 1 Batch 900 Loss 1.2505\n",
      "Epoch 1 Batch 1000 Loss 1.0632\n",
      "Epoch 1 Batch 1100 Loss 1.1993\n",
      "Epoch 1 Batch 1200 Loss 1.4308\n",
      "Epoch 1 Batch 1300 Loss 1.3235\n",
      "Epoch 1 Batch 1400 Loss 1.4057\n",
      "Epoch 1 Batch 1500 Loss 1.0359\n",
      "Epoch 1 Batch 1600 Loss 1.2740\n",
      "Epoch 1 Loss 1.2960\n",
      "Time taken for 1 epoch 1213.1575500965118 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.2128\n",
      "Epoch 2 Batch 100 Loss 1.1558\n",
      "Epoch 2 Batch 200 Loss 1.2166\n",
      "Epoch 2 Batch 300 Loss 1.1250\n",
      "Epoch 2 Batch 400 Loss 1.0359\n",
      "Epoch 2 Batch 500 Loss 1.0926\n",
      "Epoch 2 Batch 600 Loss 1.1168\n",
      "Epoch 2 Batch 700 Loss 1.2800\n",
      "Epoch 2 Batch 800 Loss 1.1035\n",
      "Epoch 2 Batch 900 Loss 0.9014\n",
      "Epoch 2 Batch 1000 Loss 0.9486\n",
      "Epoch 2 Batch 1100 Loss 0.7936\n",
      "Epoch 2 Batch 1200 Loss 0.8904\n",
      "Epoch 2 Batch 1300 Loss 1.0038\n",
      "Epoch 2 Batch 1400 Loss 0.8260\n",
      "Epoch 2 Batch 1500 Loss 0.7886\n",
      "Epoch 2 Batch 1600 Loss 1.0947\n",
      "Epoch 2 Loss 1.0664\n",
      "Time taken for 1 epoch 1209.8229072093964 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.0278\n",
      "Epoch 3 Batch 100 Loss 0.8415\n",
      "Epoch 3 Batch 200 Loss 0.7247\n",
      "Epoch 3 Batch 300 Loss 0.8824\n",
      "Epoch 3 Batch 400 Loss 0.7702\n",
      "Epoch 3 Batch 500 Loss 1.2348\n",
      "Epoch 3 Batch 600 Loss 0.8403\n",
      "Epoch 3 Batch 700 Loss 0.8968\n",
      "Epoch 3 Batch 800 Loss 0.9742\n",
      "Epoch 3 Batch 900 Loss 1.2001\n",
      "Epoch 3 Batch 1000 Loss 0.9358\n",
      "Epoch 3 Batch 1100 Loss 1.0455\n",
      "Epoch 3 Batch 1200 Loss 0.8965\n",
      "Epoch 3 Batch 1300 Loss 0.6754\n",
      "Epoch 3 Batch 1400 Loss 0.8384\n",
      "Epoch 3 Batch 1500 Loss 0.7382\n",
      "Epoch 3 Batch 1600 Loss 0.9300\n",
      "Epoch 3 Loss 0.9699\n",
      "Time taken for 1 epoch 1211.8611583709717 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.8014\n",
      "Epoch 4 Batch 100 Loss 0.8846\n",
      "Epoch 4 Batch 200 Loss 1.0213\n",
      "Epoch 4 Batch 300 Loss 1.1301\n",
      "Epoch 4 Batch 400 Loss 0.8070\n",
      "Epoch 4 Batch 500 Loss 0.7158\n",
      "Epoch 4 Batch 600 Loss 0.9029\n",
      "Epoch 4 Batch 700 Loss 0.7172\n",
      "Epoch 4 Batch 800 Loss 0.8761\n",
      "Epoch 4 Batch 900 Loss 0.9883\n",
      "Epoch 4 Batch 1000 Loss 0.9202\n",
      "Epoch 4 Batch 1100 Loss 0.8103\n",
      "Epoch 4 Batch 1200 Loss 0.8444\n",
      "Epoch 4 Batch 1300 Loss 1.0203\n",
      "Epoch 4 Batch 1400 Loss 0.9919\n",
      "Epoch 4 Batch 1500 Loss 0.8897\n",
      "Epoch 4 Batch 1600 Loss 0.8554\n",
      "Epoch 4 Loss 0.9085\n",
      "Time taken for 1 epoch 1212.9468567371368 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.7015\n",
      "Epoch 5 Batch 100 Loss 0.8965\n",
      "Epoch 5 Batch 200 Loss 1.0471\n",
      "Epoch 5 Batch 300 Loss 0.8913\n",
      "Epoch 5 Batch 400 Loss 0.8632\n",
      "Epoch 5 Batch 500 Loss 0.7097\n",
      "Epoch 5 Batch 600 Loss 0.9665\n",
      "Epoch 5 Batch 700 Loss 0.8476\n",
      "Epoch 5 Batch 800 Loss 0.9096\n",
      "Epoch 5 Batch 900 Loss 0.8991\n",
      "Epoch 5 Batch 1000 Loss 0.9792\n",
      "Epoch 5 Batch 1100 Loss 1.0530\n",
      "Epoch 5 Batch 1200 Loss 0.8336\n",
      "Epoch 5 Batch 1300 Loss 0.9412\n",
      "Epoch 5 Batch 1400 Loss 0.8882\n",
      "Epoch 5 Batch 1500 Loss 0.9325\n",
      "Epoch 5 Batch 1600 Loss 0.8495\n",
      "Epoch 5 Loss 0.8655\n",
      "Time taken for 1 epoch 1216.081574678421 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.9343\n",
      "Epoch 6 Batch 100 Loss 0.7491\n",
      "Epoch 6 Batch 200 Loss 1.0611\n",
      "Epoch 6 Batch 300 Loss 0.9932\n",
      "Epoch 6 Batch 400 Loss 1.0357\n",
      "Epoch 6 Batch 500 Loss 0.8735\n",
      "Epoch 6 Batch 600 Loss 0.6314\n",
      "Epoch 6 Batch 700 Loss 0.6770\n",
      "Epoch 6 Batch 800 Loss 0.7542\n",
      "Epoch 6 Batch 900 Loss 0.8766\n",
      "Epoch 6 Batch 1000 Loss 0.8966\n",
      "Epoch 6 Batch 1100 Loss 0.8737\n",
      "Epoch 6 Batch 1200 Loss 0.8648\n",
      "Epoch 6 Batch 1300 Loss 1.0318\n",
      "Epoch 6 Batch 1400 Loss 1.0491\n",
      "Epoch 6 Batch 1500 Loss 0.7719\n",
      "Epoch 6 Batch 1600 Loss 0.9763\n",
      "Epoch 6 Loss 0.8323\n",
      "Time taken for 1 epoch 1214.2837965488434 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.8011\n",
      "Epoch 7 Batch 100 Loss 0.8188\n",
      "Epoch 7 Batch 200 Loss 0.9593\n",
      "Epoch 7 Batch 300 Loss 0.9057\n",
      "Epoch 7 Batch 400 Loss 0.7655\n",
      "Epoch 7 Batch 500 Loss 0.8605\n",
      "Epoch 7 Batch 600 Loss 0.5632\n",
      "Epoch 7 Batch 700 Loss 0.9277\n",
      "Epoch 7 Batch 800 Loss 0.7667\n",
      "Epoch 7 Batch 900 Loss 0.9603\n",
      "Epoch 7 Batch 1000 Loss 0.8171\n",
      "Epoch 7 Batch 1100 Loss 0.7955\n",
      "Epoch 7 Batch 1200 Loss 0.9101\n",
      "Epoch 7 Batch 1300 Loss 0.6978\n",
      "Epoch 7 Batch 1400 Loss 0.4850\n",
      "Epoch 7 Batch 1500 Loss 0.9191\n",
      "Epoch 7 Batch 1600 Loss 0.8648\n",
      "Epoch 7 Loss 0.8051\n",
      "Time taken for 1 epoch 1210.9922406673431 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.7490\n",
      "Epoch 8 Batch 100 Loss 0.6412\n",
      "Epoch 8 Batch 200 Loss 0.6827\n",
      "Epoch 8 Batch 300 Loss 0.7297\n",
      "Epoch 8 Batch 400 Loss 0.9187\n",
      "Epoch 8 Batch 500 Loss 0.5908\n",
      "Epoch 8 Batch 600 Loss 0.9358\n",
      "Epoch 8 Batch 700 Loss 0.6158\n",
      "Epoch 8 Batch 800 Loss 1.0509\n",
      "Epoch 8 Batch 900 Loss 0.8892\n",
      "Epoch 8 Batch 1000 Loss 0.8098\n",
      "Epoch 8 Batch 1100 Loss 0.5490\n",
      "Epoch 8 Batch 1200 Loss 0.7561\n",
      "Epoch 8 Batch 1300 Loss 0.7409\n",
      "Epoch 8 Batch 1400 Loss 0.6197\n",
      "Epoch 8 Batch 1500 Loss 0.6690\n",
      "Epoch 8 Batch 1600 Loss 0.7746\n",
      "Epoch 8 Loss 0.7828\n",
      "Time taken for 1 epoch 1210.8799929618835 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.8973\n",
      "Epoch 9 Batch 100 Loss 0.6595\n",
      "Epoch 9 Batch 200 Loss 0.8224\n",
      "Epoch 9 Batch 300 Loss 0.6407\n",
      "Epoch 9 Batch 400 Loss 0.7677\n",
      "Epoch 9 Batch 500 Loss 0.6757\n",
      "Epoch 9 Batch 600 Loss 0.7009\n",
      "Epoch 9 Batch 700 Loss 0.7765\n",
      "Epoch 9 Batch 800 Loss 0.6287\n",
      "Epoch 9 Batch 900 Loss 0.7865\n",
      "Epoch 9 Batch 1000 Loss 0.6497\n",
      "Epoch 9 Batch 1100 Loss 0.7110\n",
      "Epoch 9 Batch 1200 Loss 1.0005\n",
      "Epoch 9 Batch 1300 Loss 0.9103\n",
      "Epoch 9 Batch 1400 Loss 0.9293\n",
      "Epoch 9 Batch 1500 Loss 0.7285\n",
      "Epoch 9 Batch 1600 Loss 0.7368\n",
      "Epoch 9 Loss 0.7639\n",
      "Time taken for 1 epoch 1217.680195569992 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.6168\n",
      "Epoch 10 Batch 100 Loss 0.9145\n",
      "Epoch 10 Batch 200 Loss 0.6851\n",
      "Epoch 10 Batch 300 Loss 0.8282\n",
      "Epoch 10 Batch 400 Loss 0.9247\n",
      "Epoch 10 Batch 500 Loss 0.6061\n",
      "Epoch 10 Batch 600 Loss 0.8572\n",
      "Epoch 10 Batch 700 Loss 0.8696\n",
      "Epoch 10 Batch 800 Loss 0.6941\n",
      "Epoch 10 Batch 900 Loss 0.8038\n",
      "Epoch 10 Batch 1000 Loss 0.7913\n",
      "Epoch 10 Batch 1100 Loss 0.6821\n",
      "Epoch 10 Batch 1200 Loss 0.8877\n",
      "Epoch 10 Batch 1300 Loss 0.9476\n",
      "Epoch 10 Batch 1400 Loss 0.6466\n",
      "Epoch 10 Batch 1500 Loss 0.6132\n",
      "Epoch 10 Batch 1600 Loss 0.6589\n",
      "Epoch 10 Loss 0.7478\n",
      "Time taken for 1 epoch 1219.7971725463867 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "RESTORE = True\n",
    "\n",
    "def training(restore):\n",
    "    if restore == False:\n",
    "        for epoch in range(EPOCHS):\n",
    "            start = time.time()\n",
    "    \n",
    "            hidden = encoder.initialize_hidden_state()\n",
    "            total_loss = 0\n",
    "    \n",
    "            for (batch, (inp, targ)) in enumerate(dataset):\n",
    "                loss = 0\n",
    "    \n",
    "                with tf.GradientTape() as tape:\n",
    "                    enc_output, enc_hidden = encoder(inp, hidden)\n",
    "    \n",
    "                    dec_hidden = enc_hidden\n",
    "    \n",
    "                    dec_input = tf.expand_dims([targ_lang.word2idx['<start>']] * BATCH_SIZE, 1)       \n",
    "    \n",
    "                    # Teacher forcing - feeding the target as the next input\n",
    "                    for t in range(1, targ.shape[1]):\n",
    "                        # passing enc_output to the decoder\n",
    "                        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "    \n",
    "                        loss += loss_function(targ[:, t], predictions)\n",
    "    \n",
    "                        # using teacher forcing\n",
    "                        dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "    \n",
    "                batch_loss = (loss / int(targ.shape[1]))\n",
    "    \n",
    "                total_loss += batch_loss\n",
    "    \n",
    "                variables = encoder.variables + decoder.variables\n",
    "    \n",
    "                gradients = tape.gradient(loss, variables)\n",
    "    \n",
    "                optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "                if batch % 100 == 0:\n",
    "                    print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                                 batch,\n",
    "                                                                 batch_loss.numpy()))\n",
    "            # saving (checkpoint) the model every 2 epochs\n",
    "            if (epoch + 1) % 2 == 0:\n",
    "                checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "            \n",
    "            print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / N_BATCH))\n",
    "            print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "    \n",
    "            with tf.contrib.summary.record_summaries_every_n_global_steps(1):\n",
    "                tf.contrib.summary.scalar('loss', total_loss / N_BATCH)\n",
    "    else:\n",
    "        # Restore the latest checkpoint and test\n",
    "        checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "training(RESTORE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    \n",
    "    inputs = [inp_lang.word2idx[w] if w in inp_lang.word2idx else inp_lang.word2idx['<unk>'] for w in tokenizer(sentence).split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word2idx['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        # storing the attention weigths to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.multinomial(predictions, num_samples=1)[0][0].numpy()\n",
    "        \n",
    "        if predicted_id in targ_lang.idx2word:\n",
    "            result += targ_lang.idx2word[predicted_id]\n",
    "        else:\n",
    "            predicted_id = targ_lang.word2idx['<unk>']\n",
    "\n",
    "        if targ_lang.idx2word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore the latest checkpoint and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    result, sentence, attention_plot = evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
    "        \n",
    "    sentence = re.sub(r'<start>|<end>|<pad>', '', sentence)\n",
    "    result = re.sub(r'<start>|<end>|<pad>', '', result)\n",
    "    \n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted answer: {}'.format(result))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question):\n",
    "    result = generate(question, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing():\n",
    "    \n",
    "    scores = []\n",
    "    bleu = sentence_bleu\n",
    "    smoothie = SmoothingFunction().method4\n",
    "\n",
    "    for (inp_row, targ_row) in zip(input_tensor_val, target_tensor_val):\n",
    "        question = ''\n",
    "        answer = ''\n",
    "        \n",
    "        for (q, a) in zip(inp_row, targ_row):\n",
    "            question += inp_lang.idx2word[q]\n",
    "            answer += targ_lang.idx2word[a]\n",
    "        \n",
    "        predicted = generate(question, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
    "        score = 0\n",
    "        \n",
    "        try:\n",
    "            score = bleu(answer, predicted, smoothing_function=smoothie)\n",
    "        except ZeroDivisionError:\n",
    "            score = 0\n",
    "    \n",
    "        scores.append(score)\n",
    "        \n",
    "#         print('question: {}'.format(question))\n",
    "#         print('actual: {}'.format(answer))\n",
    "#         print('predicted: {}'.format(predicted))\n",
    "#         print('score: {}'.format(score))\n",
    "#         print('\\n')\n",
    "        \n",
    "    belu_score = np.mean(scores) * 100\n",
    "    print('final BELU score: {}'.format(belu_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(20):\n",
    "#     ask('あなたの家のトイレにまつわるルールを教えてください。 ')\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_randomly(num_examples=1):\n",
    "    \n",
    "    score = 0\n",
    "    bleu = sentence_bleu\n",
    "    smoothie = SmoothingFunction().method4\n",
    "\n",
    "    dfr = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    for index, row in dfr.iterrows():\n",
    "        if index == num_examples: break\n",
    "        \n",
    "        question = row['question']\n",
    "        answer = row['answer']\n",
    "        \n",
    "        predicted = generate(question, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
    "\n",
    "        try:\n",
    "            score = bleu(answer, predicted, smoothing_function=smoothie)\n",
    "        except ZeroDivisionError:\n",
    "            score = 0\n",
    "\n",
    "    return question, answer, predicted, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
