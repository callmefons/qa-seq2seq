{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1.10.1\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pandas.io import gbq\n",
    "from google.cloud import bigquery\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import MeCab\n",
    "\n",
    "print(tf.executing_eagerly())\n",
    "print(tf.__version__)\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load data from google BigQuery\n",
    "\n",
    "# project_id = 'robot-personnel'\n",
    "# query = \"\"\" SELECT * FROM qa_data.free_answer WHERE gender == '女性' \"\"\"\n",
    "\n",
    "# dataset = gbq.read_gbq(query, project_id)\n",
    "# dataset.to_csv('./data/male.csv', sep=',', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'theme_id', 'question', 'answer_val', 'answer_id', 'age',\n",
       "       'gender', 'prefecture'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "df = pd.read_csv('./data/male.csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>theme_id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>prefecture</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>いい夫婦</td>\n",
       "      <td>Q1.あなたが理想とする“いい夫婦”のイメージに当てはまるものをお選びください。(MA)16...</td>\n",
       "      <td>Hがある</td>\n",
       "      <td>38</td>\n",
       "      <td>男性</td>\n",
       "      <td>鹿児島県</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>いい夫婦</td>\n",
       "      <td>Q1.あなたが理想とする“いい夫婦”のイメージに当てはまるものをお選びください。(MA)16...</td>\n",
       "      <td>これ程理想と現実のギャップが大きい物はない。</td>\n",
       "      <td>48</td>\n",
       "      <td>男性</td>\n",
       "      <td>長野県</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>いい夫婦</td>\n",
       "      <td>Q1.あなたが理想とする“いい夫婦”のイメージに当てはまるものをお選びください。(MA)16...</td>\n",
       "      <td>主義主張が共通</td>\n",
       "      <td>72</td>\n",
       "      <td>男性</td>\n",
       "      <td>長野県</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>いい夫婦</td>\n",
       "      <td>Q1.あなたが理想とする“いい夫婦”のイメージに当てはまるものをお選びください。(MA)16...</td>\n",
       "      <td>会計はお任せ</td>\n",
       "      <td>89</td>\n",
       "      <td>男性</td>\n",
       "      <td>岡山県</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>いい夫婦</td>\n",
       "      <td>Q1.あなたが理想とする“いい夫婦”のイメージに当てはまるものをお選びください。(MA)16...</td>\n",
       "      <td>性交渉</td>\n",
       "      <td>47</td>\n",
       "      <td>男性</td>\n",
       "      <td>静岡県</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  theme_id                                           question  \\\n",
       "0     いい夫婦  Q1.あなたが理想とする“いい夫婦”のイメージに当てはまるものをお選びください。(MA)16...   \n",
       "1     いい夫婦  Q1.あなたが理想とする“いい夫婦”のイメージに当てはまるものをお選びください。(MA)16...   \n",
       "2     いい夫婦  Q1.あなたが理想とする“いい夫婦”のイメージに当てはまるものをお選びください。(MA)16...   \n",
       "3     いい夫婦  Q1.あなたが理想とする“いい夫婦”のイメージに当てはまるものをお選びください。(MA)16...   \n",
       "4     いい夫婦  Q1.あなたが理想とする“いい夫婦”のイメージに当てはまるものをお選びください。(MA)16...   \n",
       "\n",
       "                   answer  age gender prefecture  \n",
       "0                    Hがある   38     男性       鹿児島県  \n",
       "1  これ程理想と現実のギャップが大きい物はない。   48     男性        長野県  \n",
       "2                 主義主張が共通   72     男性        長野県  \n",
       "3                  会計はお任せ   89     男性        岡山県  \n",
       "4                     性交渉   47     男性        静岡県  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select column and rename\n",
    "df = df[['theme_id', 'question', 'answer_val', 'age', 'gender', 'prefecture']]\n",
    "df = df.rename(columns={'answer_val': 'answer'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference : https://gist.github.com/ryanmcgrath/982242\n",
    "# UNICODE RANGE : DESCRIPTION \n",
    "# 3000-303F : punctuation\n",
    "# 3040-309F : hiragana\n",
    "# 30A0-30FF : katakana\n",
    "# FF00-FFEF : Full-width roman + half-width katakana\n",
    "# 4E00-9FAF : Common and uncommon kanji\n",
    "\n",
    "def clean_question(text):\n",
    "    text = re.split(r'[。]', text)\n",
    "    text = clean_text(text[0]+'。')\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    unicode = u\"([^\\u3000-\\u303F\\u3040-\\u309F\\u30A0-\\u30FF\\uFF00-\\uFFEF\\u4E00-\\u9FAF])\"\n",
    "    text = re.sub(unicode, \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>theme_id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>prefecture</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>いい夫婦</td>\n",
       "      <td>あなたが理想とするいい夫婦のイメージに当てはまるものをお選びください。</td>\n",
       "      <td>がある</td>\n",
       "      <td>38</td>\n",
       "      <td>男性</td>\n",
       "      <td>鹿児島県</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>いい夫婦</td>\n",
       "      <td>あなたが理想とするいい夫婦のイメージに当てはまるものをお選びください。</td>\n",
       "      <td>これ程理想と現実のギャップが大きい物はない。</td>\n",
       "      <td>48</td>\n",
       "      <td>男性</td>\n",
       "      <td>長野県</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>いい夫婦</td>\n",
       "      <td>あなたが理想とするいい夫婦のイメージに当てはまるものをお選びください。</td>\n",
       "      <td>主義主張が共通</td>\n",
       "      <td>72</td>\n",
       "      <td>男性</td>\n",
       "      <td>長野県</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>いい夫婦</td>\n",
       "      <td>あなたが理想とするいい夫婦のイメージに当てはまるものをお選びください。</td>\n",
       "      <td>会計はお任せ</td>\n",
       "      <td>89</td>\n",
       "      <td>男性</td>\n",
       "      <td>岡山県</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>いい夫婦</td>\n",
       "      <td>あなたが理想とするいい夫婦のイメージに当てはまるものをお選びください。</td>\n",
       "      <td>性交渉</td>\n",
       "      <td>47</td>\n",
       "      <td>男性</td>\n",
       "      <td>静岡県</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  theme_id                             question                  answer  age  \\\n",
       "0     いい夫婦  あなたが理想とするいい夫婦のイメージに当てはまるものをお選びください。                     がある   38   \n",
       "1     いい夫婦  あなたが理想とするいい夫婦のイメージに当てはまるものをお選びください。  これ程理想と現実のギャップが大きい物はない。   48   \n",
       "2     いい夫婦  あなたが理想とするいい夫婦のイメージに当てはまるものをお選びください。                 主義主張が共通   72   \n",
       "3     いい夫婦  あなたが理想とするいい夫婦のイメージに当てはまるものをお選びください。                  会計はお任せ   89   \n",
       "4     いい夫婦  あなたが理想とするいい夫婦のイメージに当てはまるものをお選びください。                     性交渉   47   \n",
       "\n",
       "  gender prefecture  \n",
       "0     男性       鹿児島県  \n",
       "1     男性        長野県  \n",
       "2     男性        長野県  \n",
       "3     男性        岡山県  \n",
       "4     男性        静岡県  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning the questions and answers\n",
    "df['question'] = df['question'].apply(lambda x: clean_question(str(x)))\n",
    "df['answer'] = df['answer'].apply(lambda x: clean_text(str(x)))\n",
    "\n",
    "# remove empty cell\n",
    "filter = df[\"answer\"] != \"\"\n",
    "df = df[filter].reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>theme_id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>prefecture</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>いい夫婦</td>\n",
       "      <td>あなたが理想とするいい夫婦のイメージに当てはまるものをお選びください。</td>\n",
       "      <td>がある</td>\n",
       "      <td>38</td>\n",
       "      <td>男性</td>\n",
       "      <td>鹿児島県</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>いい夫婦</td>\n",
       "      <td>あなたが理想とするいい夫婦のイメージに当てはまるものをお選びください。</td>\n",
       "      <td>これ程理想と現実のギャップが大きい物はない。</td>\n",
       "      <td>48</td>\n",
       "      <td>男性</td>\n",
       "      <td>長野県</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>いい夫婦</td>\n",
       "      <td>あなたが理想とするいい夫婦のイメージに当てはまるものをお選びください。</td>\n",
       "      <td>主義主張が共通</td>\n",
       "      <td>72</td>\n",
       "      <td>男性</td>\n",
       "      <td>長野県</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>いい夫婦</td>\n",
       "      <td>あなたが理想とするいい夫婦のイメージに当てはまるものをお選びください。</td>\n",
       "      <td>会計はお任せ</td>\n",
       "      <td>89</td>\n",
       "      <td>男性</td>\n",
       "      <td>岡山県</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>いい夫婦</td>\n",
       "      <td>あなたが理想とするいい夫婦のイメージに当てはまるものをお選びください。</td>\n",
       "      <td>性交渉</td>\n",
       "      <td>47</td>\n",
       "      <td>男性</td>\n",
       "      <td>静岡県</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  theme_id                             question                  answer  age  \\\n",
       "0     いい夫婦  あなたが理想とするいい夫婦のイメージに当てはまるものをお選びください。                     がある   38   \n",
       "1     いい夫婦  あなたが理想とするいい夫婦のイメージに当てはまるものをお選びください。  これ程理想と現実のギャップが大きい物はない。   48   \n",
       "2     いい夫婦  あなたが理想とするいい夫婦のイメージに当てはまるものをお選びください。                 主義主張が共通   72   \n",
       "3     いい夫婦  あなたが理想とするいい夫婦のイメージに当てはまるものをお選びください。                  会計はお任せ   89   \n",
       "4     いい夫婦  あなたが理想とするいい夫婦のイメージに当てはまるものをお選びください。                     性交渉   47   \n",
       "\n",
       "  gender prefecture  \n",
       "0     男性       鹿児島県  \n",
       "1     男性        長野県  \n",
       "2     男性        長野県  \n",
       "3     男性        岡山県  \n",
       "4     男性        静岡県  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering out the questions and answers that are too short or too long\n",
    "MAX_LENGTH = 50\n",
    "df = df[df['question'].map(len) < MAX_LENGTH]\n",
    "df = df[df['answer'].map(len) < MAX_LENGTH]\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79662, 6)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "num_examples = len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "    w = clean_text(w)\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(sentence):\n",
    "    \n",
    "    sentence = sentence.split('<start>')[1]\n",
    "    sentence = sentence.split('<end>')[0]\n",
    "    \n",
    "    temp = ''\n",
    "    tagger = MeCab.Tagger('')\n",
    "    tagger.parse('') \n",
    "    \n",
    "    node = tagger.parseToNode(sentence)\n",
    "    while node:\n",
    "        word = str(node.surface)\n",
    "        node = node.next\n",
    "            \n",
    "        temp += word + ' '\n",
    "        \n",
    "    return '<start> ' + temp + ' <end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return word pairs in the format: [QUESTION, ANSWER]\n",
    "\n",
    "def create_dataset(num_examples):\n",
    "\n",
    "    word_pairs = [['' for x in range(2)] for n in range(num_examples)] \n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        word_pairs[index][0] = preprocess_sentence(row['question']) \n",
    "        word_pairs[index][1] = preprocess_sentence(row['answer'])\n",
    "        \n",
    "        if index == num_examples - 1: break\n",
    "    \n",
    "    return word_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa (e.g., 5 -> \"dad\") \n",
    "class LanguageIndex():\n",
    "    def __init__(self, lang):\n",
    "        self.lang = lang\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = set()\n",
    "\n",
    "        self.create_index()\n",
    "\n",
    "    def create_index(self):\n",
    "        for phrase in self.lang:\n",
    "            self.vocab.update(tokenizer(phrase).split(' '))\n",
    "        \n",
    "        self.vocab = sorted(self.vocab)\n",
    "        self.word2idx['<pad>'] = 0\n",
    "        self.word2idx['<unk>'] = 1\n",
    "        \n",
    "        for index, word in enumerate(self.vocab):\n",
    "            self.word2idx[word] = index + 2\n",
    "\n",
    "        for word, index in self.word2idx.items():\n",
    "            self.idx2word[index] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "def load_dataset(num_examples):\n",
    "    # creating cleaned input, output pairs\n",
    "    pairs = create_dataset(num_examples)\n",
    "\n",
    "    # index word using the class defined above    \n",
    "    inp_lang = LanguageIndex(q for q, a in pairs)\n",
    "    targ_lang = LanguageIndex(a for q, a in pairs)\n",
    "    \n",
    "    # Vectorize the input and target languages\n",
    "    # Question \n",
    "    input_tensor = [[inp_lang.word2idx[q] if q in inp_lang.word2idx else inp_lang.word2idx['<unk>'] for q in tokenizer(q).split(' ') if q] for q, a in pairs]\n",
    "    \n",
    "    # Answer \n",
    "    target_tensor = [[targ_lang.word2idx[a] if a in targ_lang.word2idx else targ_lang.word2idx['<unk>'] for a in tokenizer(a).split(' ') if a] for q, a in pairs]\n",
    "    \n",
    "    # Calculate max_length of input and output tensor\n",
    "    # Here, we'll set those to the longest sentence in the dataset\n",
    "    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
    "    \n",
    "    # Padding the input and output tensor to the maximum length\n",
    "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
    "                                                                 maxlen=max_length_inp,\n",
    "                                                                 padding='post')\n",
    "    \n",
    "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
    "                                                                  maxlen=max_length_tar, \n",
    "                                                                  padding='post')\n",
    "    \n",
    "    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limit the size of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 41, 879, 22222)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_inp, max_length_targ, len(inp_lang.vocab), len(targ_lang.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63729, 63729, 15933, 15933)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a tf.data dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 32\n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dim = 64\n",
    "units = 128\n",
    "vocab_inp_size = len(inp_lang.word2idx)\n",
    "vocab_tar_size = len(targ_lang.word2idx)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(units):\n",
    "    # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n",
    "    # the code automatically does that.\n",
    "    if tf.test.is_gpu_available():\n",
    "        return tf.keras.layers.CuDNNGRU(units, \n",
    "                                        return_sequences=True, \n",
    "                                        return_state=True, \n",
    "                                        recurrent_initializer='glorot_uniform')\n",
    "    else:\n",
    "        return tf.keras.layers.GRU(units, \n",
    "                                   return_sequences=True, \n",
    "                                   return_state=True, \n",
    "                                   recurrent_activation='sigmoid', \n",
    "                                   recurrent_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.enc_units)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)        \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        # score shape == (batch_size, max_length, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoints (Object-based saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints/male/low'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = './logs/male/low'\n",
    "\n",
    "summary_writer = tf.contrib.summary.create_file_writer(log_dir, flush_millis=10000)\n",
    "summary_writer.set_as_default()\n",
    "global_step = tf.train.get_or_create_global_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 1.7932\n",
      "Epoch 1 Batch 100 Loss 1.2243\n",
      "Epoch 1 Batch 200 Loss 1.0832\n",
      "Epoch 1 Batch 300 Loss 1.2090\n",
      "Epoch 1 Batch 400 Loss 1.3190\n",
      "Epoch 1 Batch 500 Loss 1.2611\n",
      "Epoch 1 Batch 600 Loss 1.0164\n",
      "Epoch 1 Batch 700 Loss 0.9742\n",
      "Epoch 1 Batch 800 Loss 0.8683\n",
      "Epoch 1 Batch 900 Loss 0.9833\n",
      "Epoch 1 Batch 1000 Loss 1.2968\n",
      "Epoch 1 Batch 1100 Loss 0.9693\n",
      "Epoch 1 Batch 1200 Loss 0.7810\n",
      "Epoch 1 Batch 1300 Loss 1.0588\n",
      "Epoch 1 Batch 1400 Loss 0.9034\n",
      "Epoch 1 Batch 1500 Loss 0.9391\n",
      "Epoch 1 Batch 1600 Loss 1.2390\n",
      "Epoch 1 Batch 1700 Loss 0.8322\n",
      "Epoch 1 Batch 1800 Loss 0.9326\n",
      "Epoch 1 Batch 1900 Loss 1.0420\n",
      "Epoch 1 Loss 1.0413\n",
      "Time taken for 1 epoch 1591.8021233081818 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.9056\n",
      "Epoch 2 Batch 100 Loss 0.8649\n",
      "Epoch 2 Batch 200 Loss 1.0073\n",
      "Epoch 2 Batch 300 Loss 0.7872\n",
      "Epoch 2 Batch 400 Loss 1.0179\n",
      "Epoch 2 Batch 500 Loss 0.8732\n",
      "Epoch 2 Batch 600 Loss 1.1149\n",
      "Epoch 2 Batch 700 Loss 0.8162\n",
      "Epoch 2 Batch 800 Loss 0.9608\n",
      "Epoch 2 Batch 900 Loss 0.6847\n",
      "Epoch 2 Batch 1000 Loss 0.7635\n",
      "Epoch 2 Batch 1100 Loss 0.6926\n",
      "Epoch 2 Batch 1200 Loss 0.8056\n",
      "Epoch 2 Batch 1300 Loss 0.7711\n",
      "Epoch 2 Batch 1400 Loss 1.1951\n",
      "Epoch 2 Batch 1500 Loss 0.7879\n",
      "Epoch 2 Batch 1600 Loss 0.8593\n",
      "Epoch 2 Batch 1700 Loss 0.5537\n",
      "Epoch 2 Batch 1800 Loss 0.8186\n",
      "Epoch 2 Batch 1900 Loss 0.8063\n",
      "Epoch 2 Loss 0.8731\n",
      "Time taken for 1 epoch 1589.4219467639923 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.8124\n",
      "Epoch 3 Batch 100 Loss 0.6390\n",
      "Epoch 3 Batch 200 Loss 0.6999\n",
      "Epoch 3 Batch 300 Loss 0.8540\n",
      "Epoch 3 Batch 400 Loss 1.0342\n",
      "Epoch 3 Batch 500 Loss 0.9225\n",
      "Epoch 3 Batch 600 Loss 0.7368\n",
      "Epoch 3 Batch 700 Loss 0.8418\n",
      "Epoch 3 Batch 800 Loss 0.8546\n",
      "Epoch 3 Batch 900 Loss 0.6269\n",
      "Epoch 3 Batch 1000 Loss 0.7958\n",
      "Epoch 3 Batch 1100 Loss 0.7468\n",
      "Epoch 3 Batch 1200 Loss 0.9058\n",
      "Epoch 3 Batch 1300 Loss 0.6632\n",
      "Epoch 3 Batch 1400 Loss 0.7351\n",
      "Epoch 3 Batch 1500 Loss 0.9103\n",
      "Epoch 3 Batch 1600 Loss 0.7799\n",
      "Epoch 3 Batch 1700 Loss 0.7013\n",
      "Epoch 3 Batch 1800 Loss 0.8789\n",
      "Epoch 3 Batch 1900 Loss 0.8108\n",
      "Epoch 3 Loss 0.8005\n",
      "Time taken for 1 epoch 1589.0807614326477 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.6891\n",
      "Epoch 4 Batch 100 Loss 0.7254\n",
      "Epoch 4 Batch 200 Loss 0.6505\n",
      "Epoch 4 Batch 300 Loss 0.6870\n",
      "Epoch 4 Batch 400 Loss 0.5378\n",
      "Epoch 4 Batch 500 Loss 0.8290\n",
      "Epoch 4 Batch 600 Loss 0.7028\n",
      "Epoch 4 Batch 700 Loss 0.9097\n",
      "Epoch 4 Batch 800 Loss 0.7362\n",
      "Epoch 4 Batch 900 Loss 0.6526\n",
      "Epoch 4 Batch 1000 Loss 0.7826\n",
      "Epoch 4 Batch 1100 Loss 0.6342\n",
      "Epoch 4 Batch 1200 Loss 0.7664\n",
      "Epoch 4 Batch 1300 Loss 0.8517\n",
      "Epoch 4 Batch 1400 Loss 0.6722\n",
      "Epoch 4 Batch 1500 Loss 0.6242\n",
      "Epoch 4 Batch 1600 Loss 0.7843\n",
      "Epoch 4 Batch 1700 Loss 0.5969\n",
      "Epoch 4 Batch 1800 Loss 0.6168\n",
      "Epoch 4 Batch 1900 Loss 0.7448\n",
      "Epoch 4 Loss 0.7494\n",
      "Time taken for 1 epoch 1594.6566162109375 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.5861\n",
      "Epoch 5 Batch 100 Loss 0.7597\n",
      "Epoch 5 Batch 200 Loss 0.7511\n",
      "Epoch 5 Batch 300 Loss 0.6680\n",
      "Epoch 5 Batch 400 Loss 0.7132\n",
      "Epoch 5 Batch 500 Loss 0.6675\n",
      "Epoch 5 Batch 600 Loss 0.5650\n",
      "Epoch 5 Batch 700 Loss 0.7413\n",
      "Epoch 5 Batch 800 Loss 0.7394\n",
      "Epoch 5 Batch 900 Loss 0.8457\n",
      "Epoch 5 Batch 1000 Loss 0.6367\n",
      "Epoch 5 Batch 1100 Loss 0.8666\n",
      "Epoch 5 Batch 1200 Loss 0.5697\n",
      "Epoch 5 Batch 1300 Loss 0.6783\n",
      "Epoch 5 Batch 1400 Loss 0.9749\n",
      "Epoch 5 Batch 1500 Loss 0.9389\n",
      "Epoch 5 Batch 1600 Loss 0.7198\n",
      "Epoch 5 Batch 1700 Loss 0.8444\n",
      "Epoch 5 Batch 1800 Loss 0.5584\n",
      "Epoch 5 Batch 1900 Loss 0.7535\n",
      "Epoch 5 Loss 0.7119\n",
      "Time taken for 1 epoch 1598.3047444820404 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.7018\n",
      "Epoch 6 Batch 100 Loss 0.8441\n",
      "Epoch 6 Batch 200 Loss 0.5989\n",
      "Epoch 6 Batch 300 Loss 0.5921\n",
      "Epoch 6 Batch 400 Loss 0.8222\n",
      "Epoch 6 Batch 500 Loss 0.8706\n",
      "Epoch 6 Batch 600 Loss 0.5112\n",
      "Epoch 6 Batch 700 Loss 0.7714\n",
      "Epoch 6 Batch 800 Loss 0.6968\n",
      "Epoch 6 Batch 900 Loss 0.7548\n",
      "Epoch 6 Batch 1000 Loss 0.7658\n",
      "Epoch 6 Batch 1100 Loss 1.0367\n",
      "Epoch 6 Batch 1200 Loss 0.5635\n",
      "Epoch 6 Batch 1300 Loss 0.7475\n",
      "Epoch 6 Batch 1400 Loss 0.7864\n",
      "Epoch 6 Batch 1500 Loss 0.7968\n",
      "Epoch 6 Batch 1600 Loss 0.7089\n",
      "Epoch 6 Batch 1700 Loss 0.6468\n",
      "Epoch 6 Batch 1800 Loss 0.5739\n",
      "Epoch 6 Batch 1900 Loss 0.7916\n",
      "Epoch 6 Loss 0.6824\n",
      "Time taken for 1 epoch 1606.200279712677 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.8261\n",
      "Epoch 7 Batch 100 Loss 0.7123\n",
      "Epoch 7 Batch 200 Loss 0.6848\n",
      "Epoch 7 Batch 300 Loss 0.6264\n",
      "Epoch 7 Batch 400 Loss 0.5757\n",
      "Epoch 7 Batch 500 Loss 0.6145\n",
      "Epoch 7 Batch 600 Loss 0.5436\n",
      "Epoch 7 Batch 700 Loss 0.6365\n",
      "Epoch 7 Batch 800 Loss 0.5602\n",
      "Epoch 7 Batch 900 Loss 0.7783\n",
      "Epoch 7 Batch 1000 Loss 0.8171\n",
      "Epoch 7 Batch 1100 Loss 0.5581\n",
      "Epoch 7 Batch 1200 Loss 0.6147\n",
      "Epoch 7 Batch 1300 Loss 0.6242\n",
      "Epoch 7 Batch 1400 Loss 0.6515\n",
      "Epoch 7 Batch 1500 Loss 0.6115\n",
      "Epoch 7 Batch 1600 Loss 0.7078\n",
      "Epoch 7 Batch 1700 Loss 0.5590\n",
      "Epoch 7 Batch 1800 Loss 0.7664\n",
      "Epoch 7 Batch 1900 Loss 0.7156\n",
      "Epoch 7 Loss 0.6588\n",
      "Time taken for 1 epoch 1599.1836113929749 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.5213\n",
      "Epoch 8 Batch 100 Loss 0.4531\n",
      "Epoch 8 Batch 200 Loss 0.7418\n",
      "Epoch 8 Batch 300 Loss 0.5678\n",
      "Epoch 8 Batch 400 Loss 0.6532\n",
      "Epoch 8 Batch 500 Loss 0.6077\n",
      "Epoch 8 Batch 600 Loss 0.7469\n",
      "Epoch 8 Batch 700 Loss 0.4571\n",
      "Epoch 8 Batch 800 Loss 0.5333\n",
      "Epoch 8 Batch 900 Loss 0.7769\n",
      "Epoch 8 Batch 1000 Loss 0.7135\n",
      "Epoch 8 Batch 1100 Loss 0.6979\n",
      "Epoch 8 Batch 1200 Loss 0.5932\n",
      "Epoch 8 Batch 1300 Loss 0.5903\n",
      "Epoch 8 Batch 1400 Loss 0.6563\n",
      "Epoch 8 Batch 1500 Loss 0.8621\n",
      "Epoch 8 Batch 1600 Loss 0.9169\n",
      "Epoch 8 Batch 1700 Loss 0.7925\n",
      "Epoch 8 Batch 1800 Loss 0.7099\n",
      "Epoch 8 Batch 1900 Loss 0.6651\n",
      "Epoch 8 Loss 0.6392\n",
      "Time taken for 1 epoch 1597.6717014312744 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.6003\n",
      "Epoch 9 Batch 100 Loss 0.6738\n",
      "Epoch 9 Batch 200 Loss 0.5982\n",
      "Epoch 9 Batch 300 Loss 0.6865\n",
      "Epoch 9 Batch 400 Loss 0.6636\n",
      "Epoch 9 Batch 500 Loss 0.5984\n",
      "Epoch 9 Batch 600 Loss 0.6414\n",
      "Epoch 9 Batch 700 Loss 0.5869\n",
      "Epoch 9 Batch 800 Loss 0.6814\n",
      "Epoch 9 Batch 900 Loss 0.5569\n",
      "Epoch 9 Batch 1000 Loss 0.6493\n",
      "Epoch 9 Batch 1100 Loss 0.7470\n",
      "Epoch 9 Batch 1200 Loss 0.5631\n",
      "Epoch 9 Batch 1300 Loss 0.5913\n",
      "Epoch 9 Batch 1400 Loss 0.6278\n",
      "Epoch 9 Batch 1500 Loss 0.8026\n",
      "Epoch 9 Batch 1600 Loss 0.7182\n",
      "Epoch 9 Batch 1700 Loss 0.5555\n",
      "Epoch 9 Batch 1800 Loss 0.7007\n",
      "Epoch 9 Batch 1900 Loss 0.6438\n",
      "Epoch 9 Loss 0.6228\n",
      "Time taken for 1 epoch 1598.8234932422638 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.4615\n",
      "Epoch 10 Batch 100 Loss 0.5996\n",
      "Epoch 10 Batch 200 Loss 0.7346\n",
      "Epoch 10 Batch 300 Loss 0.7312\n",
      "Epoch 10 Batch 400 Loss 0.5642\n",
      "Epoch 10 Batch 500 Loss 0.5973\n",
      "Epoch 10 Batch 600 Loss 0.4062\n",
      "Epoch 10 Batch 700 Loss 0.5958\n",
      "Epoch 10 Batch 800 Loss 0.6544\n",
      "Epoch 10 Batch 900 Loss 0.5651\n",
      "Epoch 10 Batch 1000 Loss 0.7210\n",
      "Epoch 10 Batch 1100 Loss 0.6310\n",
      "Epoch 10 Batch 1200 Loss 0.4542\n",
      "Epoch 10 Batch 1300 Loss 0.5791\n",
      "Epoch 10 Batch 1400 Loss 0.6694\n",
      "Epoch 10 Batch 1500 Loss 0.7498\n",
      "Epoch 10 Batch 1600 Loss 0.6547\n",
      "Epoch 10 Batch 1700 Loss 0.5778\n",
      "Epoch 10 Batch 1800 Loss 1.0120\n",
      "Epoch 10 Batch 1900 Loss 0.7134\n",
      "Epoch 10 Loss 0.6086\n",
      "Time taken for 1 epoch 1600.5582089424133 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "RESTORE = True\n",
    "\n",
    "def training(restore):\n",
    "    if restore == False:\n",
    "        for epoch in range(EPOCHS):\n",
    "            start = time.time()\n",
    "    \n",
    "            hidden = encoder.initialize_hidden_state()\n",
    "            total_loss = 0\n",
    "    \n",
    "            for (batch, (inp, targ)) in enumerate(dataset):\n",
    "                loss = 0\n",
    "    \n",
    "                with tf.GradientTape() as tape:\n",
    "                    enc_output, enc_hidden = encoder(inp, hidden)\n",
    "    \n",
    "                    dec_hidden = enc_hidden\n",
    "    \n",
    "                    dec_input = tf.expand_dims([targ_lang.word2idx['<start>']] * BATCH_SIZE, 1)       \n",
    "    \n",
    "                    # Teacher forcing - feeding the target as the next input\n",
    "                    for t in range(1, targ.shape[1]):\n",
    "                        # passing enc_output to the decoder\n",
    "                        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "    \n",
    "                        loss += loss_function(targ[:, t], predictions)\n",
    "    \n",
    "                        # using teacher forcing\n",
    "                        dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "    \n",
    "                batch_loss = (loss / int(targ.shape[1]))\n",
    "    \n",
    "                total_loss += batch_loss\n",
    "    \n",
    "                variables = encoder.variables + decoder.variables\n",
    "    \n",
    "                gradients = tape.gradient(loss, variables)\n",
    "    \n",
    "                optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "                if batch % 100 == 0:\n",
    "                    print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                                 batch,\n",
    "                                                                 batch_loss.numpy()))\n",
    "            # saving (checkpoint) the model every 2 epochs\n",
    "            if (epoch + 1) % 2 == 0:\n",
    "                checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "            print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / N_BATCH))\n",
    "            print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "    \n",
    "            with tf.contrib.summary.record_summaries_every_n_global_steps(1):\n",
    "                tf.contrib.summary.scalar('loss', total_loss / N_BATCH)\n",
    "    else:\n",
    "        # Restore the latest checkpoint and test\n",
    "        checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "training(RESTORE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    \n",
    "    inputs = [inp_lang.word2idx[w] if w in inp_lang.word2idx else inp_lang.word2idx['<unk>'] for w in tokenizer(sentence).split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word2idx['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        # storing the attention weigths to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.multinomial(predictions, num_samples=1)[0][0].numpy()\n",
    "        \n",
    "        if predicted_id in targ_lang.idx2word:\n",
    "            result += targ_lang.idx2word[predicted_id]\n",
    "        else:\n",
    "            predicted_id = targ_lang.word2idx['<unk>']\n",
    "\n",
    "        if targ_lang.idx2word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore the latest checkpoint and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    result, sentence, attention_plot = evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
    "        \n",
    "    sentence = re.sub(r'<start>|<end>|<pad>', '', sentence)\n",
    "    result = re.sub(r'<start>|<end>|<pad>', '', result)\n",
    "    \n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted answer: {}'.format(result))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question):\n",
    "    result = generate(question, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing():\n",
    "    \n",
    "    scores = []\n",
    "    bleu = sentence_bleu\n",
    "    smoothie = SmoothingFunction().method4\n",
    "\n",
    "    for (inp_row, targ_row) in zip(input_tensor_val, target_tensor_val):\n",
    "        question = ''\n",
    "        answer = ''\n",
    "        \n",
    "        for (q, a) in zip(inp_row, targ_row):\n",
    "            question += inp_lang.idx2word[q]\n",
    "            answer += targ_lang.idx2word[a]\n",
    "        \n",
    "        predicted = generate(question, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
    "        score = 0\n",
    "        \n",
    "        try:\n",
    "            score = bleu(answer, predicted, smoothing_function=smoothie)\n",
    "        except ZeroDivisionError:\n",
    "            score = 0\n",
    "    \n",
    "        scores.append(score)\n",
    "        \n",
    "#         print('question: {}'.format(question))\n",
    "#         print('actual: {}'.format(answer))\n",
    "#         print('predicted: {}'.format(predicted))\n",
    "#         print('score: {}'.format(score))\n",
    "#         print('\\n')\n",
    "        \n",
    "    belu_score = np.mean(scores) * 100\n",
    "    print('final BELU score: {}'.format(belu_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(20):\n",
    "#     ask('あなたの家のトイレにまつわるルールを教えてください。 ')\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
